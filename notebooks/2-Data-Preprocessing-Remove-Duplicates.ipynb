{"cells":[{"cell_type":"markdown","source":["This notebook contains the second step of data preprocessing for the MIT ADSP capstone project on facial emotion detection\n","\n","Performed here:\n","- Use image hashing to determine if any duplicate images are present in the dataset\n","- Store unique (duplicates dropped) images for subsequent data processing"],"metadata":{"id":"TIPdwXKVHIHh"}},{"cell_type":"markdown","metadata":{"id":"b_9LnbnEDfdr"},"source":["Google Colab setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SSAJUh9uDimC"},"outputs":[],"source":["# Mount the drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Set main directory\n","main_directory = '/content/drive/MyDrive/facial-emotion-detection-cnn'"]},{"cell_type":"markdown","metadata":{"id":"GJC3R9sdDl-O"},"source":["Get some helpful tools"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tU8xOTpODoMB"},"outputs":[],"source":["# Get imagehash to find duplicates in dataset (to remove duplicates before training)\n","!pip install imagehash"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfXvUv1LDunh"},"outputs":[],"source":["# Import the libraries\n","import os\n","from pathlib import Path\n","import shutil\n","import glob\n","import imagehash\n","from collections import defaultdict\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from PIL import Image"]},{"cell_type":"markdown","source":["Access 'common_functions.py' file"],"metadata":{"id":"cKW1xugtekci"}},{"cell_type":"code","source":["# Specify path to common functions python file\n","project_path = Path(os.path.join(main_directory, 'notebooks'))\n","common_functions_file = 'common_functions.py'\n","\n","# Construct the full path of common functions python file\n","full_path = project_path / common_functions_file\n","\n","# Use exec to run the file\n","exec(open(full_path).read())"],"metadata":{"id":"RzVGakV2dssD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Set location to store preprocessed unique image dataset (after duplicates are removed in this notebook below)"],"metadata":{"id":"6xBbfb4DHfoZ"}},{"cell_type":"code","source":["# Specify path to preprocessed face detection dataset\n","high_confidence_directory = os.path.join(main_directory, 'data/face_detection/high_confidence_images')\n","\n","# Define the base directory for the new unique images\n","unique_image_directory = os.path.join(main_directory, 'data/unique_images')\n","\n","# Create the new directory if it doesn't exist\n","os.makedirs(unique_image_directory, exist_ok=True)"],"metadata":{"id":"QtCvCeAVHwhb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0SSu3C1_IGIW"},"source":["Search for duplicate images in train/validation datasets (exclude test data to avoid data leakage). Calculate the hash values of all images and store in a dictionary 'master_hash_dict' which contains all hash values with their corresponding filepaths. This dictionary will record all duplicate filepaths associated with a given image hash value.\n","\n","WARNING: Slow time bottleneck. Store calculated image hash values in master dictionary with filepath"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL7nR7SZgdf_"},"outputs":[],"source":["# Function to calculate hash value for an image file\n","def calculate_hash(image_path):\n","    with Image.open(image_path) as img:\n","        return imagehash.dhash(img)\n","\n","# Directories to traverse (exclude test dataset to avoid data leakage)\n","directories = ['train', 'validation']\n","\n","# Emotion categories\n","emotions = ['happy', 'sad', 'neutral', 'surprise']\n","\n","# Master dictionary to store all hash values with duplicates\n","master_hash_dict = defaultdict(list)\n","\n","# Loop through all directories and subdirectories\n","for directory in directories:\n","    for emotion in emotions:\n","        # Directory containing the images\n","        image_directory = os.path.join(high_confidence_directory, directory, emotion)\n","\n","        # Loop through all image files in the directory\n","        for root, dirs, files in os.walk(image_directory):\n","            for filename in files:\n","                if filename.endswith((\".jpg\", \".png\")):\n","                    # Get the full path of the image file\n","                    image_path = os.path.join(root, filename)\n","\n","                    # Calculate the hash value for the image\n","                    img_hash = calculate_hash(image_path)\n","\n","                    # Append the hash value and file path to the master dictionary\n","                    master_hash_dict[img_hash].append(image_path)"]},{"cell_type":"markdown","metadata":{"id":"DX0_67QAG7rj"},"source":["Create a dictionary to store hash values and associated file_paths for any images that were found to be duplicates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFicEnUBgdlg"},"outputs":[],"source":["# Create a dictionary to store duplicate hash values and their corresponding file paths\n","duplicate_hash_dict = defaultdict(list)\n","\n","# Iterate over the master_hash_dict to filter out hash values with duplicates\n","for img_hash, file_paths in master_hash_dict.items():\n","    if len(file_paths) > 1:  # Check if the hash value has duplicates\n","        duplicate_hash_dict[img_hash] = file_paths"]},{"cell_type":"markdown","metadata":{"id":"6MRkTv9MHk3S"},"source":["Query: Are there any images that are duplicates but have different emotion class labels? It would be interesting to see if the same image is labeled with different emotions..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cV4kodGUml7D"},"outputs":[],"source":["# Dictionary to store hash values with different emotion categories\n","hash_with_diff_emotion = {}\n","\n","# Iterate through duplicate_hash_dict\n","for img_hash, file_paths in duplicate_hash_dict.items():\n","    # Get the set of emotion categories for images with this hash\n","    emotion_categories = set(os.path.basename(os.path.dirname(path)) for path in file_paths)\n","\n","    # Check if there are different emotion categories\n","    if len(emotion_categories) > 1:\n","        # Store the hash value along with the different emotion categories\n","        hash_with_diff_emotion[img_hash] = emotion_categories\n","\n","# Print hash values with different emotion categories\n","for img_hash, emotions in hash_with_diff_emotion.items():\n","    print(f\"Hash value: {img_hash}\")\n","    print(\"Emotion categories:\")\n","    for emotion in emotions:\n","        print(f\"   {emotion}\")\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IZiQwxreml_-"},"outputs":[],"source":["# Define a function to display images\n","def display_images(file_paths):\n","    num_images = len(file_paths)\n","    fig, axes = plt.subplots(1, num_images, figsize=(10*num_images, 10))\n","    for i, file_path in enumerate(file_paths):\n","        with Image.open(file_path) as img:\n","            axes[i].imshow(img)\n","            axes[i].axis('off')\n","            emotion = os.path.basename(os.path.dirname(file_path))\n","            directory = os.path.basename(os.path.dirname(os.path.dirname(file_path)))\n","            axes[i].set_title(f'img_hash: {img_hash}, dataset: {directory},{emotion}')\n","    plt.show()\n","\n","# Iterate over the img_hash values in hash_with_diff_emotion\n","for img_hash in hash_with_diff_emotion.keys():\n","    # Get the file paths associated with the img_hash from master_hash_dict\n","    file_paths = master_hash_dict[img_hash]\n","    # Display the images\n","    display_images(file_paths)"]},{"cell_type":"markdown","metadata":{"id":"vfwCLZDpz3m1"},"source":["**Observations:** Sure, it's possible to be both happy and surprised at the same time. It would be interesting to develop some nuance in the emotion classes, but that is beyond the scope of our assigned task at the moment. Since our aim is to develop a model that can exclusively differentiate between a specific set of four emotion classes, these duplicate images with different emotion labels pose a problem for our assigned task. These image labels are unreliable since they have conflicting emotion labels. They will be excluded in the training, but note that it will be interesting to revisit the quality and subjectivity of the labels.  "]},{"cell_type":"markdown","metadata":{"id":"X7lB0K_ZUrqO"},"source":["Create a new dictionary 'filtered_duplicate_hash_dict' which contains all duplicate images but excludes the images above with conflicting labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvBOVglKzxTg"},"outputs":[],"source":["# Create a new dictionary to store filtered duplicate hash values\n","filtered_duplicate_hash_dict = {}\n","\n","# Iterate over the duplicate hash dictionary\n","for img_hash, file_paths in duplicate_hash_dict.items():\n","    # Check if the hash value is not present in hash_with_diff_emotion_dhash\n","    if img_hash not in hash_with_diff_emotion:\n","        # Add the hash value and its file paths to the filtered dictionary\n","        filtered_duplicate_hash_dict[img_hash] = file_paths"]},{"cell_type":"markdown","metadata":{"id":"mkzlnvuYIhfA"},"source":["Query: Are there images that are duplicates belonging to both train and validation datasets? For example, is the same image used in train/happy and validation/happy?\n","\n","Create two new dictionaries: duplicates_diff_dict which contains duplicates across train/validation datasets, and duplicates_same_dict which contains duplicates within the same /train or /validation datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cl01N04uQbBM"},"outputs":[],"source":["# Initialize dictionaries to store duplicates across different directories and duplicates in the same directory\n","duplicates_diff_dict = defaultdict(list)\n","duplicates_same_dict = {}\n","\n","# Iterate through filtered_duplicate_hash_dict\n","for img_hash, file_paths in filtered_duplicate_hash_dict.items():\n","    # Extract the directory paths from file paths\n","    directories = [os.path.dirname(path) for path in file_paths]\n","    # Check if there are multiple unique directory paths\n","    if len(set(directories)) > 1:\n","        # Add the hash value and file paths to the duplicates_diff_directory dictionary\n","        duplicates_diff_dict[img_hash].extend(file_paths)\n","    else:\n","        # Add the hash value and file paths to the duplicates_same_dict\n","        duplicates_same_dict[img_hash] = file_paths"]},{"cell_type":"markdown","metadata":{"id":"Bt99MY7NKkzK"},"source":["Since the training and validation data are not perfectly balanced, drop the duplicates from the directory with higher representation. For example, the proportion of validation/happy is much larger than the proportion of train/happy, so drop the duplicate image from /validation for happy. For the other classes, we will drop the duplicate images from training."]},{"cell_type":"markdown","metadata":{"id":"UfSDJW30WSJF"},"source":["Create a 'selected_representatives' dictionary based on the selection/drop rules just described."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HzRtcfwSIeW2"},"outputs":[],"source":["# Initialize a new dictionary to store representative images\n","selected_representatives = {}\n","\n","# Iterate through duplicates_diff_dict\n","for img_hash, file_paths in duplicates_diff_dict.items():\n","    # Initialize variables to store representative image and its directory\n","    representative_image = None\n","    representative_directory = None\n","\n","    # Iterate through file paths to find representative image based on the specified rules\n","    for file_path in file_paths:\n","        directory = os.path.dirname(file_path)\n","        emotion = os.path.basename(os.path.dirname(file_path))\n","        if emotion == 'happy' and 'train' in directory:\n","            representative_image = file_path\n","            representative_directory = directory\n","            break\n","        elif emotion in ['sad', 'surprise', 'neutral'] and 'validation' in directory:\n","            representative_image = file_path\n","            representative_directory = directory\n","            break\n","\n","    # If representative image found, add it to the selected_representatives dictionary\n","    if representative_image is not None:\n","        selected_representatives[img_hash] = [representative_image]"]},{"cell_type":"markdown","metadata":{"id":"KbDXOyUDR_ud"},"source":["Keep one representative image from duplicates within the same directory/emotion class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UoKLQ2jeIebh"},"outputs":[],"source":["# Iterate through duplicates_same_dict\n","for img_hash, file_paths in duplicates_same_dict.items():\n","    # Add the first file path encountered for each hash value to selected_representatives\n","    selected_representatives[img_hash] = file_paths[0]"]},{"cell_type":"markdown","metadata":{"id":"CLhaNOL1SMs8"},"source":["To summarize, we now have:\n","- master_hash_dict: contains all hash values along with all file_paths associated with that hash value (many duplicates)\n","- duplicate_hash_dict: contains all hash values that have duplicate images\n","- filtered_duplicate_hash_dict: all duplicates excluding those that have conflicting emotion labels\n","- selected_representatives: unique images kept from those that were duplicates\n","\n","Keep all images from master_hash_dict that are not duplicates. For any images that are duplicates, keep the selected representative that was chosen with rules that could potentially help rebalance the dataset. Create a new dictionary 'unique_images' that will contain all cleaned, unique images that we intend to carry forward to model training. Store all unique images in the designated data directory unique_images specified at the top of this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ai6ST97AIeom"},"outputs":[],"source":["# Initialize a new dictionary to store selected images\n","unique_images = {}\n","\n","# Iterate over items in master_hash_dict\n","for img_hash, file_paths in master_hash_dict.items():\n","    if len(file_paths) == 1:\n","        # If the hash value has only one file path, copy it to the final dictionary\n","        unique_images[img_hash] = file_paths[0]\n","    elif img_hash in selected_representatives:\n","        # If the hash value has multiple file paths and it is designated in selected_representatives,\n","        # copy the designated file path to the final dictionary\n","        unique_images[img_hash] = selected_representatives[img_hash]\n","\n","# Iterate over items in unique_images\n","for img_hash, file_path in unique_images.items():\n","    # Extract the actual file path if file_path is a list\n","    if isinstance(file_path, list):\n","        file_path = file_path[0]\n","\n","    # Extract the directory and emotion from the file path\n","    directory, emotion, image_name = file_path.split('/')[-3:]\n","\n","    # Create the directory structure in the new directory\n","    unique_image_directory_path = os.path.join(unique_image_directory, directory, emotion)\n","    os.makedirs(unique_image_directory_path, exist_ok=True)\n","\n","    # Copy the image file to the new directory\n","    unique_image_path = os.path.join(unique_image_directory_path, image_name)\n","    shutil.copy(file_path, unique_image_path)"]},{"cell_type":"markdown","metadata":{"id":"YnduYDF2H3OH"},"source":["Copy all High Confidence test images to the Unique_Images test directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NznQaJkEH6dC"},"outputs":[],"source":["high_confidence_test_directory = os.path.join(high_confidence_directory, 'test')\n","unique_images_test_directory = os.path.join(unique_image_directory, 'test')\n","\n","shutil.copytree(high_confidence_test_directory, unique_images_test_directory, dirs_exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"7NDwtkrPYZcB"},"source":["Show distribution of Unique_Images dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xYGaCMhxH-t4"},"outputs":[],"source":["# Print number of files within the high-confidence image directory\n","print_number_of_files(unique_image_directory)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y3lEtC9eUnfp"},"outputs":[],"source":["# Show distribution of data in train, validation, and test set\n","show_pie_charts(unique_image_directory)"]},{"cell_type":"markdown","metadata":{"id":"ldkG4chcYcIv"},"source":["**Observations:** Approximately 1K images were removed in total during data preprocessing where either (1) a face was not detected, or (2) duplicate instances of the image were removed. This cleaning procedure has not significantly changed the overall distribution of the data."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":0}